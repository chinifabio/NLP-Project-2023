{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/fabio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the Dataframe (5749, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>filename</th>\n",
       "      <th>year</th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main-captions</td>\n",
       "      <td>MSRvid</td>\n",
       "      <td>2012test</td>\n",
       "      <td>5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           genre filename      year  index  score  \\\n",
       "0  main-captions   MSRvid  2012test      1    5.0   \n",
       "1  main-captions   MSRvid  2012test      4    3.8   \n",
       "2  main-captions   MSRvid  2012test      5    3.8   \n",
       "\n",
       "                                       sentence1  \\\n",
       "0                         A plane is taking off.   \n",
       "1                A man is playing a large flute.   \n",
       "2  A man is spreading shreded cheese on a pizza.   \n",
       "\n",
       "                                           sentence2  \n",
       "0                        An air plane is taking off.  \n",
       "1                          A man is playing a flute.  \n",
       "2  A man is spreading shredded cheese on an uncoo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def on_bad_line(values):\n",
    "    return values[:7]\n",
    "\n",
    "columns_mapping = {\n",
    "    0: 'genre',\n",
    "    1: 'filename',\n",
    "    2: 'year',\n",
    "    3: 'index',\n",
    "    4: 'score',\n",
    "    5: 'sentence1',\n",
    "    6: 'sentence2'\n",
    "}\n",
    "\n",
    "# quotings 3 = csv.QUOTE_NONE\n",
    "df = pd.read_csv('Dataset/sts-train.csv', sep=\"\\t\", on_bad_lines=on_bad_line, engine='python', header=None, encoding='utf-8', quoting=3).rename(columns=columns_mapping)\n",
    "print(f'shape of the Dataframe {df.shape}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences1 = df[\"sentence1\"]\n",
    "sentences2 = df[\"sentence2\"]\n",
    "raw_corpus = np.concatenate((sentences1, sentences2))\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    result = re.sub(f'[{punctuation}]','',sentence).lower()\n",
    "    result = re.sub('\\W', ' ', result).split()\n",
    "    return [w for w in result if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1534\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_corpus = [preprocess(sentence) for sentence in raw_corpus]\n",
    "model = Word2Vec(tokenized_corpus, vector_size=60, min_count=10, window=15, negative=10)\n",
    "print(len(model.wv))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent sentences as vector (of fixed length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('looking', 0.9973329901695251), ('large', 0.9960376024246216), ('small', 0.9959952235221863), ('camera', 0.9958369135856628), ('runs', 0.9957623481750488), ('beach', 0.9955052733421326), ('grassy', 0.995479166507721), ('blue', 0.9951849579811096), ('water', 0.994867205619812), ('cow', 0.9948633313179016)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model_wiki = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('prohertrib', 0.8611263036727905), ('france', 0.8025329113006592), ('brussels', 0.7796469926834106), ('amsterdam', 0.7769756317138672), ('french', 0.7736119031906128), ('vienna', 0.7394115328788757), ('london', 0.7294350266456604), ('berlin', 0.7261149287223816), ('rome', 0.7099411487579346), ('strasbourg', 0.7078796029090881)]\n"
     ]
    }
   ],
   "source": [
    "print(model_wiki.most_similar('paris'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "\n",
    "def vectorize(docs, embedding_model, useSum=False):\n",
    "    \"\"\"\n",
    "    docs are the tweets\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # crea una matrice vuota con righe pari al numero di tweets e colonne pari alla dimensione dell'embedding\n",
    "    vectors = np.zeros((len(docs),50))\n",
    "\n",
    "    # cicla sui tweet\n",
    "    for i in range(len(docs)):\n",
    "        tokens = re.sub(f'[{punctuation}]', '', docs[i].lower()).split() # tokenizza\n",
    "        tokens = [unidecode(t) for t in tokens if t not in stop_words]\n",
    "        embeddings = [embedding_model.get_vector(token) for token in tokens if token in embedding_model] # embedding per ogni token\n",
    "        if (len(embeddings) > 0): # unisce tutti gli embedding in base a useSum\n",
    "            if (useSum): \n",
    "                vectors[i] = sum(embeddings)\n",
    "            else:\n",
    "                vectors[i] = np.mean(embeddings, axis=0)\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11498, 11498)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "vectorized_corpus_sum = vectorize(docs=raw_corpus, embedding_model=model_wiki, useSum=True)\n",
    "vectorized_corpus_mean = vectorize(docs=raw_corpus, embedding_model=model_wiki, useSum=False)\n",
    "similarity_matrix_sum = cosine_similarity(vectorized_corpus_sum)\n",
    "similarity_matrix_mean = cosine_similarity(vectorized_corpus_mean)\n",
    "print(similarity_matrix_sum.shape)\n",
    "\n",
    "# mappatura dei valori\n",
    "similarity_matrix_sum = minmax_scale(similarity_matrix_sum, feature_range=(0, 5))\n",
    "similarity_matrix_mean = minmax_scale(similarity_matrix_mean, feature_range=(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: A woman is dancing.\n",
      "Top-30 most similar pairs:\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing. \t 1.0000\n",
      "A woman is dancing and singing with other women. \t 0.9674\n",
      "A woman is dancing with other women. \t 0.9630\n",
      "A man and woman dance. \t 0.9613\n",
      "A man is dancing. \t 0.9583\n",
      "A man is dancing. \t 0.9583\n",
      "A  man is dancing. \t 0.9583\n",
      "A man is dancing. \t 0.9583\n",
      "A man is dancing. \t 0.9583\n",
      "The man is dancing. \t 0.9583\n",
      "The man is dancing. \t 0.9583\n",
      "A man is dancing. \t 0.9583\n",
      "A man is dancing. \t 0.9583\n",
      "A man is dancing. \t 0.9583\n",
      "A man is dancing. \t 0.9583\n",
      "A man is dancing. \t 0.9583\n",
      "A woman is dancing on a stage. \t 0.9573\n",
      "A woman is dancing in a cage. \t 0.9563\n",
      "The woman is singing. \t 0.9539\n",
      "A woman is holding a dancing baby up. \t 0.9418\n"
     ]
    }
   ],
   "source": [
    "def most_similar(sentence_index):\n",
    "    all_sentence_combinations = []\n",
    "    for j in range(0, len(similarity_matrix_sum)):\n",
    "            all_sentence_combinations.append([similarity_matrix_sum[sentence_index][j], sentence_index, j])\n",
    "    all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "    print(\"Sentence: \" + raw_corpus[sentence_index])\n",
    "    print(\"Top-30 most similar pairs:\")\n",
    "    for score, sentence_index, j in all_sentence_combinations[0:30]:\n",
    "        print(\"{} \\t {:.4f}\".format(raw_corpus[j], similarity_matrix_sum[sentence_index][j]))\n",
    "\n",
    "most_similar(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666: The lady put the baby int a trashcan.\n",
      "6415: A woman puts a baby in a trash can.\n",
      "similarity sum: 3.9717728932007503; mean: 3.9717728961387406\n",
      "true score is 5.0\n"
     ]
    }
   ],
   "source": [
    "def check_similarity(i: int, j: int):\n",
    "    sim_sum = similarity_matrix_sum[i][j]\n",
    "    sim_mean = similarity_matrix_mean[i][j]\n",
    "    print(f'{i}: {raw_corpus[i]}')\n",
    "    print(f'{j}: {raw_corpus[j]}')\n",
    "    print(f'similarity sum: {sim_sum}; mean: {sim_mean}')\n",
    "\n",
    "\n",
    "i = 666\n",
    "offset = int(len(raw_corpus) / 2)\n",
    "j = i + offset\n",
    "\n",
    "check_similarity(i, j)\n",
    "\n",
    "scores_true = np.ravel(df['score'].values)\n",
    "print(f'true score is {scores_true[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabio/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġunemployment', 'Ġrate', 'Ġis', 'Ġpredicted', 'Ġto', 'Ġhave', 'Ġtick', 'ed', 'Ġup', 'Ġa', 'Ġpercentage', 'Ġpoint', 'Ġto', 'Ġ6', '.', '1', '%']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer.tokenize(\"The unemployment rate is predicted to have ticked up a percentage point to 6.1%\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
